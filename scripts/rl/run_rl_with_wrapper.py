"""
Run an RL agent in an environment with non-Markov rewards generated by a wrapper with a hidden state.
Either:
    OracleWrapper: uses a hand-coded hidden state and reward function.
    LstmWrapper:   uses a trained RLEmbeddingSpaceLSTM, RLInstanceSpaceLSTM or RLCSCInstanceSpaceLSTM.
"""
import sys; sys.path.append("src"); sys.path.append("bonfire")

import argparse
from torch import save, load, cat, min, max, mean, std
from gym import make as make_env, Wrapper

from holonav.env import HoloNav
from rlutils import make as make_agent, train # https://github.com/tombewley/rlutils
from rlutils.common.env_wrappers import DoneWipeWrapper
from rlutils.observers.observer import Observer
from pytorch_mil.train import get_default_save_path
from pytorch_mil.util import get_device

from rl_training.maps import maps
from rl_training.wrappers import OracleWrapper, LstmWrapper
from rl_training.observer import OracleObserver
from model.rl_models import RLEmbeddingSpaceLSTM, RLInstanceSpaceLSTM, RLCSCInstanceSpaceLSTM
from model.lunar_lander_models import LLEmbeddingSpaceLSTM, LLInstanceSpaceLSTM, LLCSCInstanceSpaceLSTM # NOTE: different models for LunarLander
from dataset import KeyTreasureDataset, TimerTreasureDataset, MovingTreasureDataset, ChargerTreasureDataset, LunarLanderDataset
from oracles.rl.keytreasure import KeyTreasureOracle
from oracles.rl.timertreasure import TimerTreasureOracle
from oracles.rl.movingtreasure import MovingTreasureOracle
from oracles.rl.chargertreasure import ChargerTreasureOracle
from oracles.rl.lunar_lander import LunarLanderOracle
from oracles.rl.lunar_lander_timer import LunarLanderTimerOracle
from dataset import get_dataset_path_from_name


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("task", type=str)
    parser.add_argument("reward_source", type=str)
    parser.add_argument("augment_state", type=int)
    parser.add_argument("--agent", type=str, default="dqn")
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--num_eps", type=int, default=400)
    parser.add_argument("--wandb", type=int, default=1)
    parser.add_argument("--render_freq", type=int, default=0)
    parser.add_argument("--video_freq", type=int, default=0)
    parser.add_argument("--save_trajectories", type=int, default=0)
    parser.add_argument("--num_runs", type=int, default=1)

    parser.add_argument("--repeat", type=int, default=0) # For reward_source != "oracle" only
    parser.add_argument("--decay_eps", type=int, default=200) # For DQN only
    parser.add_argument("--crash_on", type=int, default=0) # For LunarLander only
    parser.add_argument("--oob_on", type=int, default=1) # For LunarLander only

    args = parser.parse_args()

    # Get task-specific paths and classes
    norm_path = f"src/rl_training/normalisation/{args.task}.pt"
    if args.task == "key_treasure":
        map_name = "keytreasure_A"
        dataset_clz = KeyTreasureDataset
        oracle_clz = KeyTreasureOracle
        ep_length = 100
    elif args.task == "timer_treasure":
        map_name = "timertreasure"
        dataset_clz = TimerTreasureDataset
        oracle_clz = TimerTreasureOracle
        ep_length = 100
    elif args.task == "moving_treasure":
        map_name = "movingtreasure"
        dataset_clz = MovingTreasureDataset
        oracle_clz = MovingTreasureOracle
        ep_length = 100
    elif args.task == "charger_treasure":
        map_name = "chargertreasure"
        dataset_clz = ChargerTreasureDataset
        oracle_clz = ChargerTreasureOracle
        ep_length = 100
    elif args.task == "lunar_lander":
        dataset_clz = LunarLanderDataset
        oracle_clz = LunarLanderOracle
        ep_length = 500
        reward_scale = 100. # NOTE:
    elif args.task == "lunar_lander_timer":
        dataset_clz = None # TODO:
        oracle_clz = LunarLanderTimerOracle
        ep_length = 300

    P = {
        "training": {
            "task": args.task,
            "agent": args.agent,
            "reward_source": args.reward_source,
            "augment_state": bool(args.augment_state),
            "repeat": args.repeat,
            "seed": args.seed,
            "num_episodes": args.num_eps,
            "wandb_monitor": bool(args.wandb),
            "project_name": f"mil-rm_{args.task}",
            "episode_time_limit": ep_length,
            "render_freq": args.render_freq,
            "video_freq": args.video_freq,
            "video_to_wandb": True,

            "crash_on": args.crash_on, # For LunarLander only
            "oob_on": args.oob_on # For LunarLander only
        },
        "agent": {
            "sac": {
                "net_pi": [(None, 256), "R", (256, 256), "R", (256, None)],
                "net_Q": [(None, 256), "R", (256, 256), "R", (256, None)],
                "replay_capacity": int(1e5), # Size of replay memory (starts overwriting when full).
                "batch_size": 128, # Size of batches to sample from replay memory during learning.
                "lr_pi": 1e-4, # Learning rate for policy.
                "lr_Q": 1e-3, # Learning rate for state-action value function.
                "gamma": 0.99, # Discount factor.
                "alpha": 0.2, # Weighting for entropy regularisation term.
                "tau": 0.005, # Parameter for Polyak averaging of target network parameters.
                "update_freq": 1, # Number of timesteps between updates.
                
                # "net_pi": [(None, 256), "R", (256, 128), "R", (128, 64), "R", (64, None)],
                # "net_Q": [(None, 256), "R", (256, 128), "R", (128, 64), "R", (64, None)],
                # "replay_capacity": int(1e5), # Size of replay memory (starts overwriting when full).
                # "batch_size": 32, # Size of batches to sample from replay memory during learning.
                # "lr_pi": 5e-5, # Learning rate for policy.
                # "lr_Q": 5e-4, # Learning rate for state-action value function.
                # "gamma": 0.99, # Discount factor.
                # "alpha": 0.2, # Weighting for entropy regularisation term.
                # "tau": 0.005, # Parameter for Polyak averaging of target network parameters.
                # "update_freq": 1, # Number of timesteps between updates.
            },
            "ddpg": { # NOTE: Actually TD3
                "net_pi": [(None, 256), "R", (256, 256), "R", (256, None), "T"], # Tanh policy (bounded in [-1,1]).
                "net_Q": [(None, 256), "R", (256, 256), "R", (256, 1)],
                "replay_capacity": 50000, # Size of replay memory (starts overwriting when full).
                "batch_size": 128, # Size of batches to sample from replay memory during learning.
                "lr_pi": 1e-4, # Learning rate for policy.
                "lr_Q": 1e-3, # Learning rate for state-action value function.
                "gamma": 0.99, # Discount factor.
                "tau": 0.005, # Parameter for Polyak averaging of target network parameters.
                "noise_params": ("ou", 0., 0.15, 0.5, 0.05, 200), # mu, theta, sigma_start, sigma_end, decay period (episodes).
                "td3": True, # Whether or not to enable the TD3 enhancements.
                # --- If TD3 enabled ---
                "td3_noise_std": 0.2,
                "td3_noise_clip": 0.5,
                "td3_policy_freq": 2
            },
            "dqn": {
                "net_Q": [(None, 256), "R", (256, 128), "R", (128, 64), "R", (64, None)], # From https://github.com/transedward/pytorch-dqn/blob/master/dqn_model.py.
                "replay_capacity": 50000, # Size of replay memory (starts overwriting when full).
                "batch_size": 128, # Size of batches to sample from replay memory during learning.
                "lr_Q": 1e-3, # Learning rate for state-action value function.
                "gamma": 0.99, # Discount factor.
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": args.decay_eps*ep_length, # Decay period (timesteps).
                "target_update": ("soft", 0.005), # Either ("hard", decay_period) or ("soft", tau).
                "double": True, # Whether to enable double DQN variant to reduce overestimation bias.
            }
        }
    }

    continuous = args.agent not in {"dqn", "actor_critic"}

    class OobDoneWrapper(Wrapper):
        """LunarLander wrapper that only terminates episode for out-of-bounds violations"""
        def __init__(self, env):
            self.env = env
            super().__init__(env)

        def step(self, action):
            next_state, reward, _, info = self.env.step(action)
            return next_state, reward, (abs(next_state[0]) >= 1.0) or (abs(next_state[1]) >= 2.0), info

    # Create unwrapped environment
    if args.task == "lunar_lander" or args.task == "lunar_lander_timer":
        env = make_env("LunarLanderContinuous-v2" if continuous else "LunarLander-v2")
        if args.crash_on == 0:
            if args.oob_on == 1: env = OobDoneWrapper(env)
            else:                env = DoneWipeWrapper(env)
    else:
        env = HoloNav(map=maps[map_name],
                    continuous=continuous,
                    action_noise=(None,) if continuous else ("gaussian", 0.2),
                    render_mode="human" if P["training"]["render_freq"] else
                                ("rgb_array" if P["training"]["video_freq"] else False))
    env.seed(P["training"]["seed"])

    # Wrap in OracleWrapper...
    if P["training"]["reward_source"] == "oracle":
        env = OracleWrapper(env, oracle_clz(), augment_state=P["training"]["augment_state"])
        num_hidden = env.oracle.internal_state_shape[0]
    # ... or wrap in LstmWrapper
    else:
        if P["training"]["reward_source"] == "emb":
            model_clz = LLEmbeddingSpaceLSTM if "lunar_lander" in args.task else RLEmbeddingSpaceLSTM
        elif P["training"]["reward_source"] == "ins":
            model_clz = LLInstanceSpaceLSTM if "lunar_lander" in args.task else RLInstanceSpaceLSTM
        elif P["training"]["reward_source"] == "csc":
            model_clz = LLCSCInstanceSpaceLSTM if "lunar_lander" in args.task else RLCSCInstanceSpaceLSTM
        else:
            raise NotImplementedError

        model_path, _, _ = get_default_save_path(args.task, model_clz.__name__,
                                                 repeat=P["training"]["repeat"])
        print(model_path)
        model = model_clz.load_model(get_device(), model_path, dataset_clz.d_in,  dataset_clz.n_expected_dims)
        model.eval()
        # Compute or load mean and standard deviation for env_state normalisation
        try:
            norm_shift, norm_scale = load(norm_path, map_location=model.device)
        except:
            print('Computing normalisation from dataset and saving...')
            bags, _, _, _ = dataset_clz.load_data(csv_path=get_dataset_path_from_name(args.task))
            all_instances = cat(bags)
            if "lunar_lander" in args.task:
                # Midpoint-range normalisation: from LunarLanderDataset.normalise
                mn, mx = min(all_instances, dim=0)[0], max(all_instances, dim=0)[0]
                norm_shift = (mx + mn) / 2
                norm_scale = mx - mn
            else:
                # Mean-std normalisation: from OracleDataset.normalise
                norm_shift = mean(all_instances, dim=0)
                norm_scale = std(all_instances, dim=0)
            save((norm_shift, norm_scale), norm_path)
            print(' Done')
        env = LstmWrapper(env, model, norm_shift=norm_shift, norm_scale=norm_scale,
                          reward_scale=reward_scale, augment_state=P["training"]["augment_state"])
        num_hidden = env.init_hidden.shape[-1]

    P["training"]["observers"] = {"oracle": OracleObserver(oracle_clz())}

    for _ in range(args.num_runs):
        if args.save_trajectories:
            P["training"]["observers"]["saver"] = Observer(
                {"save_freq": args.num_eps},
                oracle_clz.input_names + [f"h_{i}" for i in range(num_hidden)],
                ["main_engine", "side_engine"] if continuous else ["a"],
                save_path=f"results/rl_training/{args.task}/trajectories")
        # Make an agent with the specified parameters and train
        train(make_agent(args.agent, env, P["agent"][args.agent]), P["training"])